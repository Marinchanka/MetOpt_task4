# Отчёт по задаче пошагового управления инвестиционным портфелем

## 1. Цель работы

Цель работы — разработать стратегию поэтапного управления инвестиционным портфелем (две ценные бумаги и депозит) на трёх временных этапах. Требуется составить математическую модель задачи в форме динамического программирования (ДП), выписать рекуррентное соотношение Беллмана, а также реализовать численный метод решения для поиска оптимальной стратегии, максимизирующей математическое ожидание итогового капитала.

---

## 2. Постановка задачи

Начальные значения (денежные единицы, д.е.):

* ЦБ1: 100 д.е.
* ЦБ2: 800 д.е.
* Депозит: 400 д.е.
* Наличные средства: 600 д.е.

Управляющие действия разрешены в виде покупки/продажи **пакетов**, где один пакет — четверть начальной стоимости актива:

* Пакет ЦБ1 = 25 д.е.
* Пакет ЦБ2 = 200 д.е.
* Пакет депозита = 100 д.е.

Можно покупать только на имеющиеся наличные (кредиты запрещены). Продажа увеличивает наличность.

Период состоит из **трёх этапов**, на каждом из которых реализуется один из трёх сценариев: *благоприятный*, *нейтральный*, *негативный*. Ниже приведены вероятности и коэффициенты изменения стоимости активов.

### Этап 1

| Сценарий | p    | r(CB1) | r(CB2) | r(Dep) |
| -------- | ---- | ------ | ------ | ------ |
| Благопр. | 0.60 | 1.20   | 1.10   | 1.07   |
| Нейтр.   | 0.30 | 1.05   | 1.02   | 1.03   |
| Негатив. | 0.10 | 0.80   | 0.95   | 1.00   |

### Этап 2

| Сценарий | p    | r(CB1) | r(CB2) | r(Dep) |
| -------- | ---- | ------ | ------ | ------ |
| Благопр. | 0.30 | 1.40   | 1.15   | 1.01   |
| Нейтр.   | 0.20 | 1.05   | 1.00   | 1.00   |
| Негатив. | 0.50 | 0.60   | 0.90   | 1.00   |

### Этап 3

| Сценарий | p    | r(CB1) | r(CB2) | r(Dep) |
| -------- | ---- | ------ | ------ | ------ |
| Благопр. | 0.40 | 1.15   | 1.12   | 1.05   |
| Нейтр.   | 0.40 | 1.05   | 1.01   | 1.01   |
| Негатив. | 0.20 | 0.70   | 0.94   | 1.00   |

Цель — максимизировать математическое ожидание итогового капитала после этапа 3.

---

## 3. Математическая формулировка задачи ДП

### 3.1. Состояние системы


Состояние перед этапом *t*:
$$$
s_t = (x^1_t,\; x^2_t,\; d_t,\; c_t),
$$$


где
- $(x^1_t)$ — вложения в ЦБ1,
- $(x^2_t)$ — вложения в ЦБ2,
- $(d_t)$ — сумма на депозите,
- $(c_t)$ — свободные средства.

### 3.2. Управляющее действие

Действие:
$$$
a_t = (\Delta^1_t,\; \Delta^2_t,\; \Delta^d_t),
$$$
где изменения кратны размерам пакетов (25, 200, 100 д.е.).


### 3.3. Ограничения

Недопустимость отрицательных позиций:
$$$
x^1_t + \Delta^1_t \ge 0,\quad
x^2_t + \Delta^2_t \ge 0,\quad
 d_t + \Delta^d_t \ge 0.
$$$
Запрет кредитования:
$$$
\sum_{i: \, \Delta^i_t > 0} \Delta^i_t \le c_t.
$$$
### 3.4. Переход состояния


При реализации сценария $w$:
$$$
x^1_{t+1} = r^1_t(w)(x^1_t + \Delta^1_t),
$$$

$$$
x^2_{t+1} = r^2_t(w)(x^2_t + \Delta^2_t),
$$$

$$$
d_{t+1} = r^d_t(w)(d_t + \Delta^d_t),
$$$

$$$
c_{t+1} = c_t - (\Delta^1_t + \Delta^2_t + \Delta^d_t).
$$$

### 3.5. Финальная функция дохода

$$$
V_3(s_3) = x^1_3 + x^2_3 + d_3 + c_3.
$$$

---

## 4. Рекуррентное соотношение Беллмана

$$$
V_t(s_t) = \max_{a_t \in A(s_t)} \sum_{w} p_t(w) \, V_{t+1}( f(s_t, a_t, w) ).
$$$

Где функция $f(s_t,a_t,w)$ задаёт переход состояния.

Конечное условие:

$$$
V_3(s_3) = x^1_3 + x^2_3 + d_3 + c_3.
$$$

---

## 5. Псевдокод алгоритма ДП

```
Функция V(t, state):
    если t == 3:
         вернуть финальную сумму(state)
    best = -∞
    best_action = None
    для каждого допустимого действия a:
        expected = 0
        для каждого сценария w на этапе t:
            next_state = переход(state, a, w)
            expected += p(w) * V(t+1, next_state)
        если expected > best:
            best = expected
            best_action = a
    policy[state] = best_action
    вернуть best
```

---

## 6. Диаграмма классов

* **DP_Solver**

  * свойства: сценарии, параметры пакетов, кэш, policy
  * методы: `V(t, state)`, `actions_for_state`, `transition`, `simulate`

* **State** — кортеж $(x^1, x^2, d, c)$

---

## 7. Демонстрационные примеры результатов программы

Оптимальное начальное действие, найденное программной реализацией:

* **Купить 100 д.е. ЦБ1** (4 пакета)
* **Купить 200 д.е. ЦБ2** (1 пакет)
* **Купить 200 д.е. депозита** (2 пакета)

Оптимальная итоговая стоимость портфеля:

* **≈ 2091.65 д.е.**

Пример таблицы траектории (перед каждым этапом):

| Этап | ЦБ1 |  ЦБ2 | Депозит | Наличные |           Действие | Ожид. стоимость |
|------|----:|-----:|--------:|---------:|-------------------:|----------------:|
| 1    | 100 |  800 |     400 |      600 | (+100, +200, +200) |         2091.65 |
| 2    | 250 | 1100 |     650 |      100 |    (-200, 0, -100) |         2169.00 |
| 3    |  25 | 1000 |     550 |      400 |    (-25, 200, 200) |         2040.00 |

Значения, приведённые в таблице траектории, отражают результат стратегии при одном выбранном сценарии (как правило, наиболее вероятном на каждом этапе). Эти значения не являются математическим ожиданием итогового капитала.

Оптимальная стратегия в данной задаче определяется через максимум ожидаемого дохода, то есть с учётом всех возможных сценариев и их вероятностей. Поэтому отдельные значения траектории (например, более высокий итог при конкретном благоприятном сценарии) могут превышать оптимальное ожидаемое значение, однако не влияют на выбор стратегии.

Именно математическое ожидание, а не отдельные частные исходы, является критерием оптимальности в методе динамического программирования.

---

## 8. Заключение

В работе:

* Построена математическая модель портфельного управления.
* Выписано рекуррентное соотношение Беллмана.
* Реализован численный алгоритм динамического программирования.
* Найдена оптимальная стратегия по максимизации ожидаемой итоговой стоимости портфеля.
* Получены демонстрационные результаты и таблицы.

---

